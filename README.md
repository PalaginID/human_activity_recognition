# Human Activity Recognition from Sensor Data

## Оглавление

- [Описание проекта](#описание-проекта)
- [Постановка задачи](#постановка-задачи)
- [Формат данных](#формат-данных)
- [Метрики оценки](#метрики-оценки)
- [Архитектура модели](#архитектура-модели)
- [Setup](#setup)
- [Train](#train)

---

## Описание проекта

**Human Activity Recognition** — система классификации человеческого поведения на основе данных датчиков смартфона. Проект разработан для задачи CMI Detect Behavior with Sensor Data и позволяет автоматически распознавать различные виды физической активности с использованием данных акселерометра и гироскопа. Проект основан на [решении](https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data/writeups/2nd-place-solution), занявшем 2 место на соответствующем соревновании.

**Автор**: Палагин Иван

---

## Постановка задачи

Разработка системы классификации человеческого поведения на основе данных сенсоров смартфона. Задача заключается в автоматическом распознавании различных видов физической активности (ходьба, бег, сидение, стояние и др.) с использованием данных акселерометра и гироскопа. Это необходимо для создания приложений мониторинга здоровья, фитнес-трекинга и умных систем, адаптирующихся к поведению пользователя.

---

## Формат данных

### Входные данные

Временные серии данных сенсоров с фиксированной длиной сегментов:

- **Акселерометр**: 3-осевые данные (x, y, z) в м/с²
- **Гироскоп**: 3-осевые данные (x, y, z) в рад/с
- **Ориентация**: кватернионы (x, y, z, w)
- **TOF-сенсоры**: 5 сенсоров × 64 значения каждый
- **Размерность**: `[batch_size, sequence_length, features]`

### Выходные данные

Вектор вероятностей принадлежности к классам поведения:

- **Классы**: 144 класса (комбинации ориентации, жеста и поведения в фазе 1)
- **Упрощенная классификация**: 18 классов (жесты) → 9 классов (Non-Target + 8 Target жестов)
- **Размерность**: `[batch_size, num_classes]`

---

## Метрики оценки

- **Macro F1-score** - основная метрика, так как классы сбалансированы
- **Accuracy** - дополнительная метрика для общей оценки
- **Binary F1-score** - Non-Target vs Target жесты
- **Precision и Recall** по классам - для анализа качества по каждому типу активности

**Ожидаемые значения**: F1-score > 0.95, Accuracy > 0.96 (на основе результатов победителей конкурса)

### Валидация и тест

- Стратифицированное разделение по пользователям (user-based split)
- Валидация: 10-fold cross-validation с фиксированными разбиениями
- Воспроизводимость: фиксированные random seeds и сохранение списков пользователей для каждого фолда

---

## Архитектура модели

### Основная модель

Архитектура основана на комбинации нескольких подходов:

1. **Feature Engineering**:

   - Преобразование кватернионов в 6D-ротацию
   - Расчет угловой скорости
   - Удаление гравитационной компоненты из ускорения
   - Нормализация для леворуких пользователей

2. **Модельные архитектуры**:

   - **IMUModel**: Обработка только IMU данных (LSTM + Attention)
   - **ALLModel**: Обработка IMU + TOF данных (раздельные энкодеры + fusion)
   - **Глубокие варианты**: IMUDeepModel, ALLDeepModel
   - **Упрощенные варианты**: IMUSimpleModel, ALLSimpleModel
   - **2.5D архитектура**: ALL25DModel (обработка TOF как 2D изображений)

3. **Ключевые компоненты**:
   - Bidirectional LSTM для учета временных зависимостей
   - Attention механизм для выделения важных временных отрезков
   - Phase-aware mixup аугментация
   - Cosine learning rate schedule с warmup

### Обучение

- **Аугментация данных**: phase-wise mixup, TOF masking
- **Оптимизатор**: Adam с weight decay
- **Learning rate**: Cosine schedule с warmup (10% от общего числа шагов)
- **Функция потерь**: CrossEntropyLoss с поддержкой soft labels (mixup)

---

## Setup

### Предварительные требования

- Python 3.10+
- Git
- DVC (для управления данными)

### Установка зависимостей

```bash
# 1. Клонирование репозитория
git clone https://github.com/PalaginID/human_activity_recognition.git
cd human_activity_recognition

# 2. Установка UV (если не установлен)
pip install uv

# 3. Создание виртуального окружения и установка зависимостей
uv sync
source .venv/bin/activate  # Linux/Mac
# или source .venv/Scripts/activate  # Windows

# 4. Установка pre-commit hooks
uv run pre-commit install

uv run pre-commit run -a
```

## Train

Запуск обучения запускается следующим скриптом:

```bash
uv run python -m human_activity_recognition.train
```
